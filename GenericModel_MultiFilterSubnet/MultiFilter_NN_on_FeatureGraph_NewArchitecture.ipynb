{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import pandas as pd\n",
    "import tensorflow as tf\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchNormWrapper():\n",
    "    def __init__(self, useBatchNorm = False, offset = 0, scale = 1, var_eps = 1e-3):\n",
    "        self.useBatchNorm = useBatchNorm\n",
    "        self.offset = offset\n",
    "        self.scale = scale\n",
    "        self.var_eps = var_eps\n",
    "        \n",
    "    def performBatchNorm(self, matmul_plus_bias):\n",
    "        if not self.useBatchNorm:\n",
    "            return matmul_plus_bias\n",
    "        batch_mean2, batch_var2 = tf.nn.moments(matmul_plus_bias,[0])\n",
    "        return tf.nn.batch_normalization(matmul_plus_bias, batch_mean2, batch_var2, self.offset, self.scale, self.var_eps)\n",
    "    \n",
    "class DropoutWrapper():\n",
    "    def __init__(self, useDropout = False, keep_prob = 0.9):\n",
    "        self.useDropout = useDropout\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "    def performDropout(self, matmul_plus_bias):\n",
    "        if not self.useDropout:\n",
    "            return matmul_plus_bias\n",
    "        return tf.nn.dropout(matmul_plus_bias, self.keep_prob)\n",
    "    \n",
    "class RegularizerWrapper():\n",
    "    def __init__(self, useRegularizer, scale = 0.0):\n",
    "        self.useRegularizer = useRegularizer\n",
    "        self.scale = scale\n",
    "        \n",
    "    def getRegularizer(self):\n",
    "        if not self.useRegularizer:\n",
    "            return None\n",
    "        return tf.contrib.layers.l2_regularizer(scale=self.scale)\n",
    "    \n",
    "    def toStr(self):\n",
    "        return str(self.scale)\n",
    "    \n",
    "\n",
    "ActivationFunctions = Enum('ActivationFunctions', 'relu tanh sigmoid')\n",
    "InitializerTypes = Enum('InitializerTypes','RandomNormal RandomUniform TruncatedNormal GlorotNormal GlorotUniform')\n",
    "ErrorType = Enum('ErrorType', 'RMSE MAE CrossEntropy')    \n",
    "\n",
    "class WeightInitializerWrapper():\n",
    "    def __init__(self, initializerType, \n",
    "                 rand_norm_mean = 0.0, rand_norm_stddev = 1.0, \n",
    "                 rand_uniform_min = -1.0, rand_uniform_max = 1.0,\n",
    "                 trunc_norm_mean = 0.0, trunc_norm_stddev = 1.0):\n",
    "        self.init = initializerType\n",
    "        self.rand_norm_mean = rand_norm_mean\n",
    "        self.rand_norm_stddev = rand_norm_stddev\n",
    "        self.rand_uniform_min = rand_uniform_min\n",
    "        self.rand_uniform_max = rand_uniform_max\n",
    "        self.trunc_norm_mean = trunc_norm_mean\n",
    "        self.trunc_norm_stddev = trunc_norm_stddev\n",
    "        \n",
    "    def getInitializer(self):\n",
    "        if self.init == InitializerTypes.RandomNormal:\n",
    "            return tf.random_normal_initializer(mean = self.rand_norm_mean, stddev = self.rand_norm_stddev)\n",
    "        if self.init == InitializerTypes.RandomUniform:\n",
    "            return tf.random_uniform_initializer(minval = self.rand_uniform_min, maxval = self.rand_uniform_max)\n",
    "        if self.init == InitializerTypes.TruncatedNormal:\n",
    "            return tf.truncated_normal_initializer(mean = self.trunc_norm_mean, stddev = self.trunc_norm_stddev)\n",
    "        if self.init == InitializerTypes.GlorotNormal:\n",
    "            return tf.glorot_normal_initializer()\n",
    "        if self.init == InitializerTypes.GlorotUniform:\n",
    "            return tf.glorot_uniform_initializer()\n",
    "        \n",
    "    def toStr(self):\n",
    "        if self.init == InitializerTypes.RandomNormal:\n",
    "            return 'RN' + str(self.rand_norm_stddev)\n",
    "        if self.init == InitializerTypes.RandomUniform:\n",
    "            return 'RU' + str(self.rand_uniform_min) + '-' + str(self.rand_uniform_max)\n",
    "        if self.init == InitializerTypes.TruncatedNormal:\n",
    "            return 'TN' + str(self.trunc_norm_stddev)\n",
    "        if self.init == InitializerTypes.GlorotNormal:\n",
    "            return 'GN'\n",
    "        if self.init == InitializerTypes.GlorotUniform:\n",
    "            return 'GU'\n",
    "        \n",
    "        \n",
    "class PrintInfo():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def printEpochResult(self, epoch, validationError, trainError):\n",
    "        print(\"======= AT EPOCH\", epoch, \" =========\") \n",
    "        print('train error: ', trainError)\n",
    "        print('validation error: ', validationError)\n",
    "        \n",
    "    def printSavingInfo(self, validationError):\n",
    "        print('saving lower cost weights')\n",
    "        print('At validation error: ', validationError)\n",
    "    \n",
    "    def printFinalResults(self, trainError, validationError):\n",
    "        print(\"Training Done. Printing results...\")\n",
    "        print(\"Training Error = \", trainError) \n",
    "        print(\"Validation Error = \", validationError) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiFilterSubnet():\n",
    "    \n",
    "    def __init__(self, numberOfFeatures, replicationCount, subnetStructure, learn_filter_contrib_weights,\n",
    "                 activation, regularizer, dropout, initializer, batchNorm, isVerbose = True):\n",
    "        self.numberOfFeatures = numberOfFeatures\n",
    "        self.replicationCount = replicationCount\n",
    "        self.subnetStructure = subnetStructure\n",
    "        self.numberOfHiddenLayers = len(self.subnetStructure)-2\n",
    "        self.learn_filter_contrib_weights = learn_filter_contrib_weights\n",
    "        self.activation = activation\n",
    "        self.isVerbose = isVerbose\n",
    "        self.initializer = initializer\n",
    "        self.regularizer = regularizer\n",
    "        self.batchNorm = batchNorm\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        \n",
    "    def runActivationFunction(self, matmul):\n",
    "        if self.activation == ActivationFunctions.tanh:\n",
    "            return tf.nn.tanh(matmul)\n",
    "        elif self.activation == ActivationFunctions.sigmoid:\n",
    "            return tf.nn.sigmoid(matmul)\n",
    "        return tf.nn.relu(matmul)\n",
    "    \n",
    "\n",
    "    def makeSubnetStructure(self):   \n",
    "        weights, biases = [], []\n",
    "        for i in range(self.numberOfHiddenLayers + 1):\n",
    "            weights.append(tf.get_variable('wt'+str(i), shape=[self.subnetStructure[i], self.subnetStructure[i+1]], \n",
    "                           initializer=self.initializer.getInitializer(), regularizer = self.regularizer.getRegularizer()))\n",
    "            biases.append(tf.get_variable('bs'+str(i), shape=[self.subnetStructure[i+1]],\n",
    "                                          initializer=tf.constant_initializer(0.0)))\n",
    "        return weights, biases\n",
    "    \n",
    "    \n",
    "    def build_subnet(self, subnet_input, subnetname):\n",
    "        with tf.variable_scope(subnetname, reuse=True): \n",
    "            weights, biases = self.makeSubnetStructure()\n",
    "            layer_output = [subnet_input]\n",
    "            for i in range(self.numberOfHiddenLayers):\n",
    "                matmul_plus_bias =  self.dropout.performDropout(\n",
    "                                       self.runActivationFunction(\n",
    "                                        self.batchNorm.performBatchNorm(\n",
    "                                            tf.matmul(layer_output[i], weights[i]) + biases[i]\n",
    "                                        )\n",
    "                                       )\n",
    "                                     )\n",
    "                layer_output.append(matmul_plus_bias)\n",
    "            return tf.matmul(layer_output[-1], weights[self.numberOfHiddenLayers]) + biases[self.numberOfHiddenLayers]\n",
    "    \n",
    "    \n",
    "    def build_model(self, inputs, numberOfOutputNodes = 1):\n",
    "        featureOutputs = []\n",
    "        with tf.variable_scope('globwts', reuse=True):\n",
    "            for i in range(self.numberOfFeatures):   \n",
    "                sumSubnet = [] \n",
    "                for k in range(self.replicationCount):    \n",
    "                    sumSubnet.append(self.build_subnet(inputs[i], 'subnet' + str(k)))\n",
    "                    \n",
    "                filterWeights = tf.get_variable('filterWeights', shape=[self.replicationCount,1], \n",
    "                                initializer=self.initializer.getInitializer(), regularizer = self.regularizer.getRegularizer())\n",
    "                featureOutputs.append(tf.matmul(tf.concat(sumSubnet, 1), filterWeights)) \n",
    "        featureWeights = tf.get_variable('featureWeights', shape=[self.numberOfFeatures, numberOfOutputNodes], \n",
    "                                initializer=self.initializer.getInitializer(), regularizer = self.regularizer.getRegularizer())                                \n",
    "        return tf.matmul(tf.concat(featureOutputs, 1), featureWeights)\n",
    "    \n",
    "    \n",
    "    def getBatchData(self, train_x, train_y, batchsize, batchno):\n",
    "        tx = []\n",
    "        for j in range(len(train_x)):\n",
    "            tx.append((np.array(train_x[j]))[batchno*batchsize: (batchno+1)*batchsize,:])\n",
    "        ty = train_y[batchno*batchsize: (batchno+1)*batchsize]\n",
    "        return tx, ty\n",
    "    \n",
    "    def getPermutedDataForIteration(self, train_x, train_y):\n",
    "        tx = []\n",
    "        shuffle_order = np.random.permutation(np.arange(train_y.shape[0]))\n",
    "        for j in range(len(train_x)):\n",
    "            tx.append((np.array(train_x[j]))[shuffle_order])\n",
    "        ty = train_y[shuffle_order]\n",
    "        return tx, ty\n",
    "\n",
    "\n",
    "    def RunNN(self, train_x, valid_x, test_x, train_y, valid_y, learning_rate = 0.01, max_epochs = 10000, tolerance = 1e-4,\n",
    "             batchsize = 32, errorType = ErrorType.RMSE, numberOfOutputNodes = 1):\n",
    "        tf.reset_default_graph()\n",
    "                \n",
    "        with tf.variable_scope('globwts'):\n",
    "            tf.get_variable('filterWeights', shape=[self.replicationCount,1], \n",
    "                                initializer=self.initializer.getInitializer(), regularizer = self.regularizer.getRegularizer())\n",
    "            for i in range(self.replicationCount):\n",
    "                with tf.variable_scope('subnet'+str(i)): \n",
    "                    self.makeSubnetStructure() \n",
    "                \n",
    "        with tf.name_scope(\"IO\"):\n",
    "            inputarr = []\n",
    "            for i in range(self.numberOfFeatures):\n",
    "                inputarr.append(tf.placeholder(tf.float32, [None, self.subnetStructure[0]], name=\"X\"+str(i)))\n",
    "            outputs = tf.placeholder(tf.float32, [None, numberOfOutputNodes], name=\"Yhat\")    \n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            yout = self.build_model(inputarr, numberOfOutputNodes) \n",
    "            cost_op = None\n",
    "            if errorType == ErrorType.RMSE:\n",
    "                cost_op = tf.losses.mean_squared_error(labels = outputs, predictions = yout)\n",
    "            elif errorType == ErrorType.MAE:\n",
    "                cost_op = tf.metrics.mean_absolute_error(labels = outputs, predictions = yout)\n",
    "            elif errorType == ErrorType.CrossEntropy:\n",
    "                cost_op = tf.nn.softmax_cross_entropy_with_logits(labels = outputs, logits = yout)\n",
    "            else:\n",
    "                raise ValueError('Inappropriate error type!')\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_op)\n",
    "\n",
    "        epoch = 0\n",
    "        current_min_validation_error = 1000.0\n",
    "        printInfo = PrintInfo()\n",
    "        if self.isVerbose:\n",
    "            print( \"Beginning Training\" )\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session() # Create TensorFlow session\n",
    "        saver = tf.train.Saver()\n",
    "        save_dir = 'checkpoints/'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = os.path.join(save_dir, 'best_validation1')\n",
    "        with sess.as_default():\n",
    "            sess.run(init)\n",
    "            traindict = {i:t for i,t in zip(inputarr, train_x)} \n",
    "            traindict[outputs] = train_y\n",
    "            validdict = {i:t for i,t in zip(inputarr, valid_x)} \n",
    "            validdict[outputs] = valid_y\n",
    "            testdict = {i:t for i,t in zip(inputarr, test_x)} \n",
    "            savedEpoch = -1\n",
    "            numbatch = int(train_y.shape[0]/batchsize)\n",
    "            while True:                \n",
    "                train_iter_x, train_iter_y = self.getPermutedDataForIteration(train_x, train_y)\n",
    "                for i in range(numbatch):\n",
    "                    tx, ty = self.getBatchData(train_iter_x, train_iter_y, batchsize, i)\n",
    "                    tdict = {mi:mt for mi,mt in zip(inputarr, tx)} \n",
    "                    tdict[outputs] = ty\n",
    "                    sess.run( train_op, feed_dict = tdict)\n",
    "                    #TODO: support for classification problems\n",
    "                    cost_validation = sess.run(cost_op, feed_dict = validdict)\n",
    "                    if current_min_validation_error - cost_validation > tolerance:\n",
    "                        if self.isVerbose == True:\n",
    "                            printInfo.printSavingInfo(cost_validation)\n",
    "                        saver.save(sess=sess, save_path=save_path)\n",
    "                        savedEpoch = epoch\n",
    "                        current_min_validation_error = cost_validation\n",
    "                        \n",
    "                if self.isVerbose:\n",
    "                    printInfo.printEpochResult(epoch, cost_validation, sess.run(cost_op, feed_dict = traindict))\n",
    "                if epoch > max_epochs :\n",
    "                    break   \n",
    "                epoch += 1\n",
    "\n",
    "            saver.restore(sess=sess, save_path=save_path)\n",
    "            if self.isVerbose:\n",
    "                printInfo.printFinalResults(sess.run(cost_op, feed_dict= traindict), sess.run(cost_op, feed_dict= validdict))   \n",
    "                \n",
    "            return sess.run(yout, feed_dict=testdict), \\\n",
    "                      sess.run(cost_op, feed_dict= validdict), sess.run(cost_op, feed_dict= traindict), savedEpoch\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SubnetWrapper():\n",
    "    \n",
    "    def __init__(self, num_of_features, num_of_fingerprints, replicationCount, learn_filter_contrib_weights,\n",
    "                 activation, regularizer, hiddenLayer, initializer, \n",
    "                 runcnt, dropout = 1.0, learningrate = 0.001, tolerance = 1e-4):\n",
    "        self.num_of_features = num_of_features\n",
    "        self.num_of_fingerprints = num_of_fingerprints\n",
    "        self.replicationCount = replicationCount\n",
    "        self.learn_filter_contrib_weights = learn_filter_contrib_weights\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "        self.hiddenLayer = hiddenLayer\n",
    "        self.initializer = initializer\n",
    "        self.learningrate = learningrate\n",
    "        self.runcnt = runcnt\n",
    "        self.tolerance = tolerance\n",
    "        if dropout > 0.99:\n",
    "            self.dropout = DropoutWrapper(False)\n",
    "        else:\n",
    "            self.dropout = DropoutWrapper(True, dropout)\n",
    "        self.categoricalVarIndices = dict()\n",
    "        self.numberOfClasses = 1\n",
    "    \n",
    "    def extractDataColsForTF(self, data):\n",
    "        x = []\n",
    "        for d in range(self.num_of_features):\n",
    "            x.append(data[:, d*self.num_of_fingerprints : (d+1)*self.num_of_fingerprints])\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def AppendListOfData(self, dataList):\n",
    "        if not isinstance(dataList, list):\n",
    "            if dataList.ndim == 1:\n",
    "                return dataList.reshape(dataList.shape[0], 1)\n",
    "            else:\n",
    "                return dataList\n",
    "        if len(dataList) == 0:\n",
    "            print('ERROR! Empty data!!!')\n",
    "            return None\n",
    "        for i in range(len(dataList)):\n",
    "            if dataList[i].ndim == 1:\n",
    "                dataList[i] = dataList[i].reshape(dataList[i].shape[0], 1)\n",
    "        appendedData = dataList[0]\n",
    "        for i in range(1, len(dataList)):\n",
    "            appendedData = np.vstack((appendedData, dataList[i]))\n",
    "        return appendedData\n",
    "    \n",
    "    def convertForNN(self, X_train, X_validation, X_test, Y_train, Y_validation, Y_test, energyPivot = None):\n",
    "        if Y_train.ndim == 1:\n",
    "            Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "        if Y_validation.ndim == 1:\n",
    "            Y_validation = Y_validation.reshape(Y_validation.shape[0], 1)\n",
    "        if Y_test.ndim == 1:\n",
    "            Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "        return self.extractDataColsForTF(X_train), \\\n",
    "                  self.extractDataColsForTF(X_validation),  \\\n",
    "                    self.extractDataColsForTF(X_test), \\\n",
    "                        Y_train, Y_validation, Y_test, energyPivot\n",
    "                \n",
    "    \n",
    "    def setCategoricalVariableIndices(self, y):\n",
    "        for i in range(y.shape[0]):\n",
    "            if y[i,0] not in self.categoricalVarIndices:\n",
    "                self.categoricalVarIndices[y[i,0]] = self.numberOfClasses\n",
    "                self.numberOfClasses += 1\n",
    "    \n",
    "    \n",
    "    def oneHotEncodingFor(self, y):\n",
    "        onehotY = np.zeros((y.shape[0], self.numberOfClasses))\n",
    "        for i in range(y.shape[0]):\n",
    "            onehotY[i, self.categoricalVarIndices[y[i,0]]] = 1\n",
    "        return onehotY\n",
    "    \n",
    "    \n",
    "    def getOneHotEncoding(self, Y_train, Y_validation, Y_test):\n",
    "        self.numberOfClasses = 0\n",
    "        self.setCategoricalVariableIndices(Y_train)\n",
    "        self.setCategoricalVariableIndices(Y_validation)\n",
    "        self.setCategoricalVariableIndices(Y_test)\n",
    "        oneHot_trainY = self.oneHotEncodingFor(Y_train)\n",
    "        oneHot_validationY = self.oneHotEncodingFor(Y_validation)\n",
    "        oneHot_testY = self.oneHotEncodingFor(Y_test)\n",
    "        return oneHot_trainY, oneHot_validationY, oneHot_testY\n",
    "    \n",
    "    \n",
    "    def getDataSuitableForSubnet(self, encodingTrains, energyTrains, encodingTests, energyTests, splitIndices, isCategorical):\n",
    "        encodingTrain = self.AppendListOfData(encodingTrains)\n",
    "        energyTrain = self.AppendListOfData(energyTrains)\n",
    "        encodingTest = self.AppendListOfData(encodingTests)\n",
    "        energyTest = self.AppendListOfData(energyTests)\n",
    "        shuffle_order = np.random.permutation(np.arange(energyTrain.shape[0]))\n",
    "        energyTrain = energyTrain[shuffle_order]\n",
    "        encodingTrain = encodingTrain[shuffle_order]\n",
    "        split1 = splitIndices[0]\n",
    "        split2 = splitIndices[1]\n",
    "        X_train, Y_train = encodingTrain[:split1, :], energyTrain[:split1]\n",
    "        X_validation, Y_validation = encodingTrain[split1:split2, :], energyTrain[split1:split2]\n",
    "        if isCategorical:\n",
    "            Y_train, Y_validation, energyTest = self.getOneHotEncoding(Y_train, Y_validation, energyTest)        \n",
    "        return self.convertForNN(X_train, X_validation, encodingTest, Y_train, Y_validation, energyTest)\n",
    "        \n",
    "\n",
    "    def prepareSubnetStructures(self, hiddenStructure):\n",
    "        return [self.num_of_fingerprints] + hiddenStructure + [1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def makeRepeatedInterpolatingPrdictionsWithEnsemble(self, repeatCount, testSetSize, splitIndices, max_epochs, \n",
    "                                                        encodings, energies, batchsize = 32, errorType = ErrorType.RMSE,\n",
    "                                                        isCategorical = False, useBatchNorm = False, isVerbose = True):\n",
    "        errors = []\n",
    "        netStruct = self.prepareSubnetStructures(self.hiddenLayer)\n",
    "        print(netStruct)\n",
    "        k = 0\n",
    "        while k < repeatCount:\n",
    "            i = 0\n",
    "            test_y = None\n",
    "            predsDF = pd.DataFrame(index = range(testSetSize), columns = \n",
    "                               [i for i in range(self.runcnt)] +\n",
    "                              ['Mean Pred', 'Std Pred', 'Actual', 'AE'])\n",
    "            encoding = self.AppendListOfData(encodings)\n",
    "            energy = self.AppendListOfData(energies)\n",
    "            shuffle_order = np.random.permutation(np.arange(energy.shape[0]))\n",
    "            energy = energy[shuffle_order]\n",
    "            encoding = encoding[shuffle_order]\n",
    "            split1 = splitIndices[1]\n",
    "            encoding1, energy1 = encoding[:split1, :], energy[:split1]\n",
    "            encoding2, energy2 = encoding[split1:, :], energy[split1:]\n",
    "            while i < self.runcnt:\n",
    "                train_x, valid_x, test_x, train_y, valid_y, test_y, _ = \\\n",
    "                                                                            self.getDataSuitableForSubnet( \\\n",
    "                                                                            encoding1, energy1, encoding2, \\\n",
    "                                                                            energy2, splitIndices, isCategorical)\n",
    "                tf.reset_default_graph()\n",
    "                net = MultiFilterSubnet(self.num_of_features, self.replicationCount, netStruct, \n",
    "                                        self.learn_filter_contrib_weights, self.activation, self.regularizer, self.dropout,\n",
    "                                        self.initializer, BatchNormWrapper(useBatchNorm), isVerbose = isVerbose)\n",
    "\n",
    "                preds, verr, terr, e = net.RunNN(train_x, valid_x, test_x, train_y, valid_y, \\\n",
    "                                                      learning_rate = self.learningrate, max_epochs = max_epochs, \\\n",
    "                                                      batchsize = batchsize, tolerance = self.tolerance, \n",
    "                                                      errorType = errorType, numberOfOutputNodes = self.numberOfClasses)\n",
    "                predsDF.iloc[:, i] = preds\n",
    "                #TODO: support for classification problem\n",
    "                testerror = np.mean(abs(preds - test_y))\n",
    "                if errorType == ErrorType.RMSE:\n",
    "                    testerror = np.sqrt(np.mean((preds - test_y)**2))\n",
    "                    verr = np.sqrt(verr)\n",
    "                    terr = np.sqrt(terr)\n",
    "                print('test set error', testerror)\n",
    "                print('validation error', verr)\n",
    "                print('training error', terr)\n",
    "                print('saved in epoch', e)\n",
    "                i += 1\n",
    "            predsDF.loc[:, 'Actual'] = test_y\n",
    "            predictionArray = np.array(predsDF.iloc[:,:(self.runcnt)].values, dtype=np.float32)\n",
    "            predsDF.loc[:, 'Mean Pred'] = np.mean(predictionArray, axis = 1)\n",
    "            predsDF.loc[:, 'Std Pred'] = np.std(predictionArray, axis = 1)\n",
    "            predsDF.loc[:, 'AE'] = abs(predsDF.loc[:,'Actual'] - predsDF.loc[:,'Mean Pred'])\n",
    "            error = np.mean(predsDF.loc[:, 'AE'].values)\n",
    "            if errorType == ErrorType.RMSE:\n",
    "                error = np.sqrt(np.mean(predsDF.loc[:, 'AE'].values ** 2))\n",
    "            print('Error for Ensemble:', error)\n",
    "            errors.append(error)\n",
    "            k += 1\n",
    "            \n",
    "        print('Done with mean of RMSE of %s' %(np.mean(errors)))\n",
    "        return np.mean(errors), np.std(errors)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pandas import pandas as pd\n",
    "\n",
    "\n",
    "def RunForFilters(encoding, energy, testSetSize, splitIndices, outputDir, \n",
    "                  numOfFeatures, numOfFingerprints, hiddlayers, filterNumList, \n",
    "                  runsForFilter, runsForEnsemble, errorType, isCategorical):\n",
    "    savepath = os.path.join(outputDir, 'RunResults_NewArch_NFt_' + str(numOfFeatures) + '_NFP_' + str(numOfFingerprints) +\n",
    "                            '_hidd' + str(hiddlayers) + '.csv')\n",
    "    collist = ['FilterCount','Mean of Error', 'SD of Error']\n",
    "    if not os.path.exists(savepath):\n",
    "        df = pd.DataFrame(columns = collist)\n",
    "        df.to_csv(savepath)\n",
    "\n",
    "    for k in filterNumList:\n",
    "        print('Number of Filters', str(k))\n",
    "        df = pd.read_csv(savepath, index_col=0)\n",
    "        if len(df[(df['FilterCount']==k)]) > 0:\n",
    "            print('Already computed. Continuing...')\n",
    "            continue\n",
    "        print('Computing...')\n",
    "        pred = SubnetWrapper(num_of_features = numOfFeatures, num_of_fingerprints = numOfFingerprints, \n",
    "                                 replicationCount = k, learn_filter_contrib_weights = True,\n",
    "                             activation = ActivationFunctions.tanh, regularizer = RegularizerWrapper(True, 0.001), \n",
    "                             hiddenLayer = hiddlayers, dropout = 0.9,\n",
    "                          initializer = WeightInitializerWrapper(InitializerTypes.RandomNormal, rand_norm_stddev = 0.0001), \n",
    "                             learningrate = 0.001, runcnt = runsForEnsemble, tolerance = 1e-3)\n",
    "        mean_error, sd_error = pred.makeRepeatedInterpolatingPrdictionsWithEnsemble(repeatCount = runsForFilter,\n",
    "                                                                 testSetSize = testSetSize, \n",
    "                                                                 splitIndices = splitIndices, \n",
    "                                                                 max_epochs = 10000,\n",
    "                                                                 encodings = encoding, energies = energy, \n",
    "                                                                 batchsize = 384, errorType = errorType,\n",
    "                                                                 isCategorical = isCategorical,\n",
    "                                                                 useBatchNorm = True, isVerbose = False)\n",
    "        print('Eror for this filter:', mean_error)\n",
    "        df.loc[len(df)] = dict(zip(collist, [k, mean_error, sd_error]))\n",
    "        df.to_csv(savepath)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding = np.loadtxt('regr_housing_attr13.csv', delimiter=',', skiprows=0, usecols=range(0,78))\n",
    "energy = np.loadtxt('regr_housing_attr13.csv', delimiter=',', skiprows=0, usecols=78)\n",
    "RunForFilters(encoding = encoding, energy = energy, testSetSize = 58, splitIndices = [384,448], outputDir = '', \n",
    "              numOfFeatures = 13, numOfFingerprints = 6, hiddlayers = [8,4], \n",
    "              filterNumList = [8], runsForFilter = 5, runsForEnsemble = 5, \n",
    "              errorType = ErrorType.RMSE, isCategorical = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding = np.loadtxt('ohsumed500_stat.csv', delimiter=',', skiprows=0, usecols=range(0,135))\n",
    "energy = np.loadtxt('ohsumed500_stat.csv', delimiter=',', skiprows=0, usecols=135)\n",
    "RunForFilters(encoding = encoding, energy = energy, testSetSize = 58, splitIndices = [400,440], outputDir = '', \n",
    "              numOfFeatures = 27, numOfFingerprints = 5, hiddlayers = [8,4], \n",
    "              filterNumList = [8], runsForFilter = 5, runsForEnsemble = 5, \n",
    "              errorType = ErrorType.CrossEntropy, isCategorical = True)  #TODO: support for classification not done yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This cell is used to draw the weight matrix between the input layer and the first hidden layer for each subnet (filter)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_as_image(wtmatrix, numOfFilters):\n",
    "    maxval = np.max(np.abs([wtmatrix.min(),wtmatrix.max()]))\n",
    "    plt.figure()\n",
    "    for i in range(numOfFilters):\n",
    "        plt.imshow(wtmatrix[i,:,:], cmap='RdYlGn', interpolation='nearest',\n",
    "                   vmin=-maxval, vmax=maxval)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "def makeSubnetStructure(subnetStructure):  \n",
    "    weights = []\n",
    "    for i in range(len(subnetStructure) - 1):\n",
    "        weights.append(tf.get_variable('wt'+str(i), shape=[subnetStructure[i], subnetStructure[i+1]]))\n",
    "    return weights\n",
    "\n",
    "def getLeftWeightMatrixAndDraw(numOfFilters, subnetStructure, dirToLook, fileName):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.variable_scope('globwts'):\n",
    "        for i in range(numOfFilters):\n",
    "            with tf.variable_scope('subnet'+str(i)): \n",
    "                makeSubnetStructure(subnetStructure) \n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session() # Create TensorFlow session\n",
    "    saver = tf.train.Saver()\n",
    "    save_dir = 'checkpoints/'\n",
    "    if not os.path.exists(dirToLook):\n",
    "        os.makedirs(dirToLook)\n",
    "    save_path = os.path.join(dirToLook, fileName)\n",
    "    weightMatrix = []\n",
    "    with sess.as_default():\n",
    "        sess.run(init)        \n",
    "        saver.restore(sess=sess, save_path=save_path)\n",
    "        g = tf.get_default_graph()\n",
    "        with tf.variable_scope('globwts'):\n",
    "            for i in range(numOfFilters):\n",
    "                with tf.variable_scope('subnet'+str(i), reuse=True):\n",
    "                    weights = makeSubnetStructure(subnetStructure)\n",
    "                    weightMatrix.append(weights[0].eval())\n",
    "\n",
    "    weightMatrix = np.array(weightMatrix)\n",
    "    np.save('Wtmatrix_generic', weightMatrix)\n",
    "    print(weightMatrix.shape)\n",
    "    show_as_image(weightMatrix, numOfFilters)\n",
    "    \n",
    "\n",
    "getLeftWeightMatrixAndDraw(numOfFilters = 8, subnetStructure = [6,8,4,1], dirToLook = 'checkpoints/', \n",
    "                           fileName = 'best_validation1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
